# SparkSQL_Home_Sales_Analysis

Overview

The project utilizes SparkSQL to analyze home sales data, extracting key metrics and optimizing performance through various Spark functionalities.

Project Tasks

1. File Preparation: Rename the starter code file to "Home_Sales.ipynb".
2. Import Libraries: Include necessary PySpark SQL functions.
3. Data Reading: Read the provided home sales data.
4. Temporary Table Creation: Create a temporary table for the data.
5. Answer Questions: Utilize SparkSQL to answer specific questions about the data.
6. Caching: Cache the temporary table.
7. Check Cache: Confirm if the table is cached.
8. Cached Query: Run a query using the cached data and compare runtime.
9. Data Partitioning: Partition the data by the "date_built" field.
10. Parquet Data Setup: Create a temporary table for the Parquet data.
11. Parquet Query: Run a query on the Parquet data and compare runtime.
12. Uncaching: Remove the cached table.
13. Verification: Ensure the table is uncached using PySpark.
   